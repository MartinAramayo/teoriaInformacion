---
classoption:
- twocolumn
---
# Practica 1(2021)

<!-- Libros de referencia:

[1]: Elements of information theory de T. Cover -->

## Problema 3:
**Enunciado:**
Entropía de una variable binaria: Calcule la entropia de una variable $X$ binaria que toma valores en $\mathcal{A}_{X}=\left\{x_{1}, x_{2}\right\}$ con probabilidades $\{p, 1-p\}$ y grafiquela como función de $p$. Asegúrese de entender por qué se anula en $p=0$ y $p=1$, por qué es máxima en $p=1 / 2 \mathrm{y}$ por qué es simétrica.

**Resolucion:**
X es una variable binaria tal que :
$$
X = \left\{ 
\begin{aligned}
1 & \text{ Probabilidad } p \\
0 & \text{ Probabilidad } 1-p
\end{aligned} \right.
$$

La variable $X$ tiene una entropía:
$$
H(X) = -p \log(p) - (1-p) \log (1-p)
$$

Para $p=0$ y $p=1$ no hay incerteza por lo que la entropía es nula. Por otro lado donde se tiene máxima incertidumbre de la variable en $p=1/2$, donde no se puede aprovechar que un valor de $X$ sea mas probable que otro para reducir el numero de preguntas necesarias para obtener su valor, esto implica una mayor incerteza en el valor de la variable.

## Problema 4: 
**Enunciado:**
Entropía de una variable de distribución plana: Calcule la entropia de una variable $X$ que puede tomar $M$ valores en $\mathcal{A}_{X}=\left\{x_{1}, x_{2}, \ldots, x_{M}\right\}$, de tal forma que todos $\operatorname{los} x_{i}$ son equiprobables. ¿Qué sucede en el limite de $M \rightarrow \infty$ ?

**Resolucion:**
### Entropía de una variable de distribución plana:

Dada una variable $X$ que puede tomar $M$ valores en $\mathcal{A}_{X}=\left\{x_{1}, x_{2}, \ldots, x_{M}\right\}$, de tal forma que todos  los $x_{i}$ son equiprobables.
$$
X = \left\{ 
\begin{matrix}
x_1, x_2, \ldots, x_M, & \text{Con probabilidad } \frac{1}{M} \\
\end{matrix} \right.
$$
$$
H = - \sum_{i=1}^{M} P \log_M P; \quad P=\frac{1}{M}
$$
$$
H = - \log_M \frac{1}{M} = 1 \text{ M-its}
$$

Puede reescribirse en \texttt{bits}:
$$
H = \log_2 M \text{ bits}
$$

A medida que aumenta la cantidad de variables aumenta el numero medio de preguntas binarias que hay que realizar para obtener el valor de la variable. Crece de forma logarítmica (mas lento que lineal). En el limite $M \rightarrow \infty$ se requiere un numero infinito de preguntas binarias para conocer el valor de la variable.

<!-- Calcule la entropía de una variable $X$ que puede tomar $M$ valores en $\mathcal{A}_{X}=\left\{x_{1}, x_{2}, \ldots, x_{M}\right\}$, de tal forma que todos $\operatorname{los} x_{i}$ son equiprobables. ¿Qué sucede en el límite de $M \rightarrow \infty$ ? -->

## Problema 5:
**Enunciado:**
Entropía de una distribución de potencia: Se tira una moneda (no cargada) tantas veces como
haga falta, hasta que salga la primera cara. $\operatorname{Si} X$ es el número de tiros que hacen falta, encuentre
a) La entropia $H(X)$ en bits,
b) una estrategia de preguntas binarias eficiente. Compare el numero medio de preguntas con el resultado de (a).
¿Cómo se modifica el problema si la moneda está cargada?
21

**Resolucion:**
Problema Coin Flips en [1]:

Primero resolvamos el problema para una moneda cargada, 
$$
Y = \left\{ 
\begin{matrix}
H & \text{ probabilidad } p \\
T & \text{ probabilidad } q=1-p \\
\end{matrix} \right.
$$

a su vez, la variable $X$, que indica cuantas veces tuve que medir $Y$ para obtener $H$, tiene alfabeto $A_X$ igual a $\mathbb{N}$. Como p es la probabilidad de obtener una moneda en $H$, la probabilidad de sacar $H$ en el $n$-esimo tiro es la de no sacar $H$ unas $n-1$ veces. Por lo que la probabilidad viene dada por:
$$
X = \left\{ 
n \in \mathbb{N} \quad \text{ probabilidad } \underbrace{q^{n-1}}_{\text{fallo}}
\underbrace{p}_{\text{exito}} 
\right .
$$

Calculando la entropia en bits:
$$
\begin{aligned}
H(X) =& - \sum_{k=1}^\infty p_k \log_2 p_k \\
=& - \sum_{k=1}^\infty q^{k-1} p \log_2 (q^{k-1} p) \\
=& - \sum_{k=0}^\infty q^{k} p \log_2 (q^{k} p)  \\
=& - \sum_{k=0}^\infty q^{k} p \log_2 q^{k} - \sum_{k=0}^\infty q^{k} p \log_2 p  \\
=& - p \log_2 q \sum_{k=0}^\infty k q^{k}
   - p \log_2 p \sum_{k=0}^\infty q^{k}  \\
=& - \frac{q}{(1-q)^2} p \log_2 q 
   - \frac{1}{1-q} p \log_2 p  \\
=& - \frac{1-p}{p} \log_2 (1-p)
   - \log_2 p  \\
\end{aligned}
$$

Usando que : 
$$
\sum_{n=0}^{\infty} r^{n}
=\frac{1}{1-r} , \sum_{n=1}^{\infty} r^{n}=\frac{r}{1-r}, 
\quad 
\sum_{n=1}^{\infty} n r^{n}
=\frac{r}{(1-r)^{2}}
$$

De la expresion de $H(X)$ se puede ver que para $p=1/2$, $H(X)=2$.

Respecto a una estrategia de preguntas que permita obtener el valor de $X$, puede hacerse las preguntas:

¿Es 1?, ¿Es 2?, ¿Es 3?, ..., ¿Es n?

Analizando $H(X)$, para $p=1$, siempre tenemos $H$ por lo que $H(X)=0$, a medida que baja $p$ debo hacer mas preguntas para obtener el valor de $X$, esto se ve en la expresion de $H(X)$ que crece a medida que $p$ se acerca a $p=0$, en el caso limite $p \rightarrow 0$ se ve que  $H(X) \rightarrow \infty$.

## Problema 6:
**Enunciado:**
Demuestre que $H(X, Y \mid Z)=H(X \mid Z)+H(Y \mid X, Z)$

**Resolucion:**
Usando: $H(X,Y) = H(X) + H(Y \mid X)$
$$
\begin{aligned}
H(X, Y, Z) &= H(X, Z) + H(Y \mid X, Z)  \\
H(X, Y, Z) &= H(Z) + H(X| Z)+H(Y| X, Z) \\
H(X, Y \mid Z)+H(Z) &= H(Z)+H(X \mid Z)+H(Y \mid X, Z) \\
\end{aligned}
$$
$$
\boxed{H(X, Y \mid Z) = H(X \mid Z)+H(Y \mid X, Z)}
$$


## Problema 7:
**Enunciado:**
${ }_{6}$ Es necesariamente $H(X \mid Y)=H(Y \mid X)$ ? En caso afirmativo, demuéstrelo. En caso negativo, construya un contracjemplo.

**Resolucion:**
Sabiendo:
$$
\begin{aligned}
H(X, Y) &= H(X) + H(Y \mid X) \\
H(X, Y) &= H(Y) + H(X \mid Y) 
\end{aligned}
$$

Se tiene que:
$$
H(X \mid Y) = H(Y \mid X) + H(X) - H(Y)
$$

Si $H(X) \neq H(Y)$ no es posible. Un ejemplo en el que las entropias son distintas es cuando se tienen $X$ dada por una moneda justa e $Y$ por una moneda cargada. Para este caso $H(X \mid Y) \neq H(Y \mid X)$.

## Problema 8:
**Enunciado:**
Entropías infinitas: En clase se demostró que si $X \in \mathcal{A} x$ puede variar entre un número $|\mathcal{A} x|$ (cardinal de $A_{X}$ ) de posibilidades, entonces $H(X) \leq \log _{2}\left|\mathcal{A}_{X}\right|$. Esta desigualdad no prohibe que $H(X)$ diverja, si $\mathcal{A}_{X}$ es un conjunto de infinitos elementos (por el momento, consideraremos sólo infinitos numerables). En el problema 3 vimos un ejemplo en el que $H(X)$ permanecía finita, a pesar de que $\left|\mathcal{A}_{X}\right|$ divergia. Sin embargo, ésto no tiene por qué ser siempre asi. Si $\left|\mathcal{A}_{X}\right|=$ $\infty, H(X)$ puede permanecer finita, o puede diverger. Demuéstrelo, analizando el caso $\mathcal{A}_{X}=$ $\{2,3,4, \ldots\}$, donde $p\left(n \in \mathcal{A}_{X}\right)=\left(\mathrm{a} n \ln ^{2} n\right)^{-1}$, donde $\alpha$ es una constante de normalización

definida por
$$
\alpha=\sum_{n=2}^{+\infty}\left(n \ln ^{2} n\right)^{-1}
$$
Es fácil ver que $\alpha$ es finito, ya que la suma infinita se puede acotar por la integral de $\left(x \ln ^{2} x\right)^{-1}$

**Resolucion:**
Calculamos la entropia para la variable aleatoria $X$, donde $A_X = \left\lbrace n>2: n \in \mathbb{N} \right\rbrace$. Donde las probabilidades vienen dadas por $p(n \in A_X) = ( \alpha n \ln ^2 n ) ^{-1}$ con $\alpha = \sum_{n=2}^\infty( n \ln ^2 n ) ^{-1}$:
$$
\begin{aligned}
H(X) &= - \sum_{n=2}^\infty p_n \ln p_n \\
H(X) &= -\sum_{n=2}^\infty \frac{\ln ( \alpha n \ln ^2 n ) ^{-1}}{( \alpha n \ln ^2 n )}  \\
H(X) &= \sum_{n=2}^\infty \frac{\ln ( \alpha n \ln ^2 n ) }{( \alpha n \ln ^2 n )} \\
H(X) &= \sum_{n=2}^\infty \frac{ \ln ( \alpha ) + \ln ( n) + \ln( \ln ^2 n ) }{( \alpha n \ln ^2 n )} \\
H(X) &= \sum_{n=2}^\infty \frac{ \ln ( \alpha ) }{( \alpha n \ln ^2 n )}  + \sum_{n=2}^\infty  \frac{ \ln ( n)}{( \alpha n \ln ^2 n )} + \sum_{n=2}^\infty \frac{ \ln( \ln ^2 n ) }{( \alpha n \ln ^2 n )} \\
H(X) &= \sum_{n=2}^\infty \frac{ \ln ( \alpha ) }{( \alpha n \ln ^2 n )}  + \sum_{n=2}^\infty  \frac{ 1}{( \alpha n \ln n )} + \sum_{n=2}^\infty \frac{ \ln( \ln ^2 n ) }{( \alpha n \ln ^2 n )} \\
\end{aligned}
$$

Analizando el termino del medio:
$$
\sum_{n=2}^\infty  \frac{ 1}{( \alpha n \ln n )} 
> \int_2^\infty \frac{ 1}{( \alpha n \ln n )} 
$$
$$
= \left. \frac{1}{\alpha} \ln(u) \right|_2^\infty
$$

## Problema 9:
**Enunciado:**
Regla de la cadena generalizada: Demuestre que
$$
H\left(X_{1}, \ldots, X_{N}\right)=\sum_{i=1}^{N} H\left(X_{i} \mid X_{1}, \ldots, X_{i-1}\right)
$$

**Resolucion:**
$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
$$

Demo: 
$$
\begin{aligned}
H\left(X_{1}, X_{2}\right) &=H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right) \\
H\left(X_{1}, X_{2}, X_{3}\right) &=H\left(X_{1}\right)+H\left(X_{2}, X_{3} \mid X_{1}\right) \\
&=H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right)+H\left(X_{3} \mid X_{2}, X_{1}\right) \\
&\vdots \\
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)&=H\left(X_{1}\right)+H\left(X_{2} \mid X_{1}\right) \\
&+\ldots+H\left(X_{n} \mid X_{n-1}, \ldots, X_{1}\right) \\
&=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right) . \quad \square
\end{aligned}
$$

Obtuvimos así la primera relación de la regla de la cadena. La segunda se obtiene de manera análoga,
intercambiando las variables $X$ e $Y$.

\newcommand{\mean}[1]{\left\langle {#1} \right\rangle}

$$
\begin{aligned}
   \mean{ \log{x}}  \leq log {\mean x} \\
   \frac{1}{N}\sum_{i=1}^N \log{x_i} \leq log \mean{x} \\
   \log{\sqrt[N]{\prod_{i=1}^N x_i}} \leq log \mean{x} \\
\end{aligned}
$$

