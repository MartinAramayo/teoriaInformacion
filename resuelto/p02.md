---
classoption:
- twocolumn
---
# Practica 2(2021)

<!-- Libros de referencia:

[1]: Elements of information theory de T. Cover -->


## Problema 1:
**Enunciado:**
Demuestre que la divergencia de Kullback-Leibler puede no ser simétrica y no cumplir con la desigualdad triangular. ${ }_{i}$ En qué casos la divergencia de Kullback-Leibler se va a infinito?

**Resolucion:**
Contraejemplo:
Sistema de monedas cargadas:
$$
\begin{aligned}
   x_1 \in A_{X_1} &= \left\lbrace H,T \right\rbrace , \text{Prob de } H = p_H =1/10  \\
   x_2 \in A_{X_2} &= \left\lbrace H,T \right\rbrace , \text{Prob de } H = q_H =2/10 \\
   x_3 \in A_{X_3} &= \left\lbrace H,T \right\rbrace , \text{Prob de } H = r_H =3/10 \\
\end{aligned}
$$

La divergencia para dos monedas cargadas con distinto peso $p$ y $q$ de $H$:
$$
D_{KL} (p \mid\mid q) = p \log \frac{p}{q}
+ (1-p) \log \frac{(1-p)}{(1-q)}
$$

Para la desigualdad triangular:
$$
D_{KL} (p \mid\mid q) + D_{KL} (q \mid\mid r)  \approx 0,0271
$$
$$
D_{KL} (p \mid\mid r) \approx 0.0370
$$

NO satisface la desigualdad triangular
$$
D_{KL} (p \mid\mid q) + D_{KL} (q \mid\mid r)  \approx 0,0271
\leq D_{KL} (p \mid\mid r) 
$$


Para simetría:
$$
D_{KL} (p \mid\mid q) \approx 0.037, \quad
D_{KL} (q \mid\mid p) \approx 0.019
$$

son distintas, $D_{KL}$ es simétrica.

## Problema 2:

**Enunciado:**
Utilizando la desigualdad de Jensen, demuestre que una variable aleatoria es determinista si y sólo si su varianza se anula.

**Resolucion:**
Un variable aleatoria es determinista $\Leftrightarrow$ Su varianza es nula

Demo:

como $f(x)= x^2$ es estrictamente convexa:

$$
X \text{ es det } \Leftrightarrow f( \left\langle X \right\rangle ) = \left\langle f(X) \right\rangle \Leftrightarrow Var = \left\langle X^2 \right\rangle - \left\langle X \right\rangle^2=0
$$

## Problema 3

**Enunciado:**
Demuestre que el promedio aritmético de un conjunto de nümeros reales positivos es mayor o igual que su promedio geométrico.
**Resolucion:**
Demuestre que el promedio aritmético de un conjunto de números reales positivos es mayor o
igual que su promedio geométrico.

Demo:

\newcommand{\mean}[1]{\left\langle {#1} \right\rangle}

$$
\begin{aligned}
   \mean{ \log x_i } &\leq \log \mean{x_i} \\   
   \frac{1}{N}\sum{ \log x_i } &\leq \log \mean{x_i} \\   
   \frac{1}{N}{ \log \prod x_i } &\leq \log \mean{x_i} \\   
   { \log \sqrt[n]{\prod x_i} } &\leq \log \mean{x_i} \\   
   { \sqrt[n]{\prod x_i} } &\leq  \mean{x_i} \\   
   { \sqrt[n]{\prod x_i} } &\leq  \frac{1}{N}\sum{x_i} \\   
\end{aligned}
$$

que es lo que se buscaba

## Problema 4

**Enunciado:**
Demuestre que la entropia es una función cóncava de $p$. Es decir, si se consideran dos distribuciones $p_{1}$ y $p_{2}$, entonces se cumple que para todo $\lambda \in[0,1]$
$$
H\left[\lambda p_{1}+\{1-\lambda) p_{2}\right] \geq \lambda H\left(p_{1}\right)+(1-\lambda) H\left(p_{2}\right) .
$$
**Resolucion:**
Theorem: The cross-entropy is convex in the probability distribution $q$, i.e.
$$
\mathrm{H}\left[p, \lambda q_{1}+(1-\lambda) q_{2}\right] \leq \lambda \mathrm{H}\left[p, q_{1}\right]+(1-\lambda) \mathrm{H}\left[p, q_{2}\right]
$$
where $p$ is a fixed and $q_{1}$ and $q_{2}$ are any two probability distributions and $0 \leq \lambda \leq 1$.
Proof: The relationship between Kullback-Leibler divergence, entropy and cross-entropy is:
$$
\mathrm{KL}[P \| Q]=\mathrm{H}(P, Q)-\mathrm{H}(P)
$$
Note that the $\mathrm{KL}$ divergence is convex in the pair of probability distributions $(p, q)$ :
$$
\mathrm{KL}\left[\lambda p_{1}+(1-\lambda) p_{2} \| \lambda q_{1}+(1-\lambda) q_{2}\right] \leq \lambda \mathrm{KL}\left[p_{1} \| q_{1}\right]+(1-\lambda) \mathrm{KL}\left[p_{2} \| q_{2}\right]
$$
A special case of this is given by
$$
\begin{aligned}
\mathrm{KL}\left[\lambda p+(1-\lambda) p \| \lambda q_{1}+(1-\lambda) q_{2}\right] & \leq \lambda \mathrm{KL}\left[p \| q_{1}\right]+(1-\lambda) \mathrm{KL}\left[p \| q_{2}\right] \\
\mathrm{KL}\left[p \| \lambda q_{1}+(1-\lambda) q_{2}\right] & \leq \lambda \mathrm{KL}\left[p \| q_{1}\right]+(1-\lambda) \mathrm{KL}\left[p \| q_{2}\right]
\end{aligned}
$$
and applying equation $(2)$, we have
$$
\begin{aligned}
\mathrm{H}\left[p, \lambda q_{1}+(1-\lambda) q_{2}\right]-\mathrm{H}[p] & \leq \lambda\left(\mathrm{H}\left[p, q_{1}\right]-\mathrm{H}[p]\right)+(1-\lambda)\left(\mathrm{H}\left[p, q_{2}\right]-\mathrm{H}[p]\right) \\
\mathrm{H}\left[p, \lambda q_{1}+(1-\lambda) q_{2}\right]-\mathrm{H}[p] & \leq \lambda \mathrm{H}\left[p, q_{1}\right]+(1-\lambda) \mathrm{H}\left[p, q_{2}\right]-\mathrm{H}[p] \\
\mathrm{H}\left[p, \lambda q_{1}+(1-\lambda) q_{2}\right] & \leq \lambda \mathrm{H}\left[p, q_{1}\right]+(1-\lambda) \mathrm{H}\left[p, q_{2}\right]
\end{aligned}
$$
which is equivalent to $(1)$.

## Problema 5

**Enunciado:**
Cota de la independencia: Demuestre que
$$
H\left(X_{1}, X_{2}, \ldots, X_{N}\right) \leq \sum_{i} H\left(X_{i}\right)
$$
**Resolucion:**
Usando 
$$
H\left(X_{1}, X_{2}, \ldots, X_{n}\right)=\sum_{i=1}^{n} H\left(X_{i} \mid X_{i-1}, \ldots, X_{1}\right)
$$

Sabemos que:

$$
H(X \mid Y) \leq H(X)
$$

Cuando X e Y son independientes:
generalizando: